{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83a780-675b-4a26-b9e5-cf6a18489544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries to run the code\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from IPython.display import clear_output\n",
    "from gensim.test.utils import datapath\n",
    "import pyLDAvis.gensim as gensimvis\n",
    "from gensim.models import LdaModel\n",
    "import gensim.corpora as corpora\n",
    "from pymongo import MongoClient\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pyLDAvis\n",
    "import requests\n",
    "import pymongo\n",
    "import gensim\n",
    "import tweepy\n",
    "import string\n",
    "import numpy\n",
    "import spacy\n",
    "import emoji\n",
    "import nltk\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1012c239-5ecd-4bef-8807-67510ba8c3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup database and tables names\n",
    "dbname = 'vaccinitaly'\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "tweet_collection = client[dbname]['labeled_tweets']\n",
    "complete_collection = client[dbname]['vaccines_tweets']\n",
    "no_vax_users = client[dbname]['no_vax_users']\n",
    "no_vax_users_cleaned = client[dbname]['no_vax_users_cleaned']\n",
    "last_100_no_vax_tweets_cleaned = client[dbname]['last_100_no_vax_tweets_per_user_cleaned']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9c591e-2026-4dc1-bd1c-1b4db7702a29",
   "metadata": {},
   "source": [
    "<h3>1.1 - Retrieve the users associated with the classified tweets without users</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc63598-8311-4ffe-94b0-2faa795579b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tweet_user(tweet_id, main_collection, user):\n",
    "    query = {\"id\": tweet_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"user\": user\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8ff97-01df-4359-9394-0565bd61f942",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_tweets = tweet_collection.find({\"user\": None}, no_cursor_timeout = True)\n",
    "all_labeled_tweets_count = tweet_collection.count_documents({\"user\": None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb2f86f-a68f-420c-aece-ee5fb8064b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve users from the complete collection to complete the labeled collection\n",
    "tweet_processed_count = 0\n",
    "for tweet in all_labeled_tweets:\n",
    "    \n",
    "    complete_tweet = complete_collection.find_one({\"id\": tweet['id']})\n",
    "    \n",
    "    if complete_tweet != None:\n",
    "        update_tweet_user(tweet['id'], tweet_collection, complete_tweet['user'])\n",
    "    \n",
    "    tweet_processed_count += 1\n",
    "    print(\"Pre-Processed ({:.2f} %) -\".format(float(tweet_processed_count)/float(all_labeled_tweets_count) * 100), tweet_processed_count,\"/\", all_labeled_tweets_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f5687e-a74d-498c-be63-19cb9340f923",
   "metadata": {},
   "source": [
    "<h3>1.2 - Collect from Twitter the users associated with the <b>classified</b> tweets without users</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc82f6-17ab-4904-a2c9-38f51ab3bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(\"\", \"\")\n",
    "auth.set_access_token(\"\", \"\")\n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3dcf42-0e4c-4aa9-9b21-387f9d9883d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_look(tweet_id, main_collection, bool_):\n",
    "    query = {\"id\": tweet_id}\n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"looked_for_user\": bool_\n",
    "        }\n",
    "    }\n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117840cd-8b8d-47f1-bc7e-87d24b473941",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_tweets = tweet_collection.find({}, no_cursor_timeout = True)\n",
    "labeled_tweets_count = tweet_collection.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee77f998-553f-40e6-af74-8e5daa55e2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup whether the user has been found or not\n",
    "processed_count = 0\n",
    "for tweet in labeled_tweets:\n",
    "    if 'user' in tweet and tweet['user'] != None:\n",
    "        update_user_look(tweet['id'], tweet_collection, True)\n",
    "    else:\n",
    "        update_user_look(tweet['id'], tweet_collection, False)\n",
    "    \n",
    "    processed_count += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_count)/float(labeled_tweets_count) * 100), processed_count,\"/\", labeled_tweets_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efe9699-a52a-45c3-a94f-231055328876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tweet_user(tweet_id, main_collection, user):\n",
    "    query = {\"id\": tweet_id}\n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"user\": user,\n",
    "            \"looked_for_user\": True\n",
    "        }\n",
    "    }\n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9235694b-cd48-4732-8a9d-aeca9234e730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_missing_users():\n",
    "    labeled_tweets_without_user = tweet_collection.find({\"user\": None, \"looked_for_user\": False}, no_cursor_timeout = True)\n",
    "    labeled_tweets_without_user_count = tweet_collection.count_documents({\"user\": None, \"looked_for_user\": False})\n",
    "    \n",
    "    processed_count = 0\n",
    "\n",
    "    for tweet in labeled_tweets_without_user:\n",
    "        try:\n",
    "            retr_tweet = api.get_status(tweet['id'])\n",
    "            save_tweet_user(tweet['id'], tweet_collection, retr_tweet.user._json)\n",
    "        except:\n",
    "            update_user_look(tweet['id'], tweet_collection, True)\n",
    "            processed_count += 1\n",
    "            print(\"Processed ({:.2f} %) -\".format(float(processed_count)/float(labeled_tweets_without_user_count) * 100), processed_count,\"/\", labeled_tweets_without_user_count)\n",
    "            clear_output(wait=True)\n",
    "            continue\n",
    "        \n",
    "        processed_count += 1\n",
    "        print(\"Processed ({:.2f} %) -\".format(float(processed_count)/float(labeled_tweets_without_user_count) * 100), processed_count,\"/\", labeled_tweets_without_user_count)\n",
    "        clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27f0b15-0d96-419a-9dcd-a712239c3908",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    collect_missing_users()\n",
    "except:\n",
    "    collect_missing_users()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983b17e1-1ab5-4458-977b-cbe650d9dd83",
   "metadata": {},
   "source": [
    "<h3>1.3 - Build the collection of no-vax users</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac69c45-423b-4868-996c-03fafa9f8b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_no_vax_percentage(user_id, main_collection, no_vax_percentage):\n",
    "    query = {\"id\": user_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"no_vax_percentage\": no_vax_percentage\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae851fce-c820-4be5-be0f-bfef7cad9b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labeled_tweets = tweet_collection.find({\"user\": {\"$ne\": None}})\n",
    "all_labeled_tweets_count = tweet_collection.count_documents({\"user\": {\"$ne\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e77b79-d429-4b82-9a74-62d3f335368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# golden hashtags\n",
    "neutral = [\"vaccini\", \"vaccino\", \"vaccinazioni\", \"vaccinazione\", \"novaccinoainovax\", \"vaccinocovid\", \"vaccinoanticovid\"]\n",
    "pro_vax = [\"vaccinarsi\", \"vaccinerai\", \"vaccinare\", \"vaccineremo\", \"vacciniamoci\", \"vaccinerete\", \"vaccinareh24\", \"vaccinerò\"]\n",
    "no_vax = [\"iononmivaccino\", \"iononmivaccinero\", \"iononsonounacavia\"]\n",
    "\n",
    "no_vax_limit = 0.8\n",
    "tweet_processed_count = 0\n",
    "no_vax_user_count = 0\n",
    "no_vax_users_detected = []\n",
    "analysed_users = []\n",
    "\n",
    "for tweet in all_labeled_tweets:\n",
    "    # ignore if already analysed\n",
    "    if tweet['user']['id'] in analysed_users:\n",
    "        continue\n",
    "    \n",
    "    # check if the tweet could be a no-vax one depending on the classification keepinig only the predictions with score > 0.8\n",
    "    all_pred = [tweet['pred1'], tweet['pred2'], tweet['pred3'], tweet['pred4'], tweet['pred5']]\n",
    "    no_vax_majority = sum(float(pred) > no_vax_limit for pred in all_pred)\n",
    "    \n",
    "    # if so\n",
    "    if no_vax_majority > round(len(all_pred)/2, 0):\n",
    "        # add the user to the list of already analysed users\n",
    "        analysed_users.append(tweet['user']['id'])\n",
    "        \n",
    "        no_vax_users.insert_one(tweet['user'])\n",
    "        \n",
    "        count_all = 0\n",
    "        count_no_vax = 0\n",
    "        \n",
    "        # collect all the tweets\n",
    "        no_vax_user_tweets = tweet_collection.find({'user.id': tweet['user']['id']})\n",
    "        \n",
    "        for user_tweet in no_vax_user_tweets:\n",
    "            is_no_vax = False\n",
    "            \n",
    "            # if it contains at least one hashtag\n",
    "            if len(user_tweet['hashtags']) > 0:\n",
    "                count_all += 1\n",
    "\n",
    "                # compute if only one of those hashtag is a golden one or not\n",
    "                for no_vax_hash in no_vax:\n",
    "                    if no_vax_hash in user_tweet['hashtags']:\n",
    "                        is_no_vax = True\n",
    "                        break\n",
    "\n",
    "                # check majority on prediction scores\n",
    "                all_pred = [user_tweet['pred1'], user_tweet['pred2'], user_tweet['pred3'], user_tweet['pred4'], user_tweet['pred5']]\n",
    "                no_vax_majority = sum(float(pred) > no_vax_limit for pred in all_pred)\n",
    "\n",
    "                if no_vax_majority > round(len(all_pred)/2, 0):\n",
    "                    is_no_vax = True\n",
    "\n",
    "                # increase no-vax count\n",
    "                if is_no_vax:\n",
    "                    count_no_vax += 1\n",
    "\n",
    "        # compute \"no-vaxity\" percentage\n",
    "        no_vax_perc = 0\n",
    "        if count_all != 0:\n",
    "            no_vax_perc = round(float(count_no_vax)/float(count_all),2)\n",
    "    \n",
    "        # update user\n",
    "        update_user_no_vax_percentage(tweet['user']['id'], no_vax_users, no_vax_perc)\n",
    "\n",
    "    # count processed tweets\n",
    "    tweet_processed_count += 1\n",
    "    print(\"Pre-Processed ({:.2f} %) -\".format(float(tweet_processed_count)/float(all_labeled_tweets_count) * 100), tweet_processed_count,\"/\", all_labeled_tweets_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84484d57-facf-4a96-b65b-30b57b6a6d6b",
   "metadata": {},
   "source": [
    "<h3>2 - Collect the last 100 tweets shared by the users classified as no-vax</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac6997-d764-46e4-ac8f-16ca30f24b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup content for tweet collection\n",
    "bearer_token = \"\"\n",
    "field_list = \"attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,public_metrics,possibly_sensitive,referenced_tweets,reply_settings,source,text,withheld\"\n",
    "search_url = \"https://api.twitter.com/2/tweets/search/all\"\n",
    "\n",
    "tweet_limit = 100\n",
    "last_100_no_vax_tweets = client['vaccinitaly']['last_' + str(tweet_limit) + '_no_vax_tweets_per_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7795194-3e13-4fa9-87e2-dc743ca7a234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1507ccbe-9eb5-4a5e-b994-ee74591cbf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_to_endpoint(search_url, headers, params):\n",
    "    response = requests.request(\"GET\", search_url, headers=headers, params=params)\n",
    "\n",
    "    return response.json(), response.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba5aac5-0cec-498b-9087-4a3cb0c1d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_as_processed(tweet_id, main_collection):\n",
    "    query = {\"id\": tweet_id}\n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"processed\": True\n",
    "        }\n",
    "    }\n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f41965-5dbb-42a4-b9cc-c0c04ce82dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_count = no_vax_users.count_documents({})\n",
    "user_ids = no_vax_users.find({}, no_cursor_timeout = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56107651-4b79-4a33-91fe-b79cf6e8836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_collected = 0\n",
    "processed_count = 0\n",
    "\n",
    "headers = create_headers(bearer_token)\n",
    "\n",
    "# collect last 100 tweets for users\n",
    "for user in user_ids:\n",
    "    current_collected = 0\n",
    "    processed = False\n",
    "\n",
    "    if 'processed' not in user or not user['processed']:\n",
    "\n",
    "        mark_as_processed(user['id'], no_vax_users)\n",
    "\n",
    "        if last_100_no_vax_tweets.count_documents({\"author_id\": str(user['id'])}) == 0:\n",
    "            processed = True\n",
    "            from_ = 'from: {}'.format(user['id'])\n",
    "            query_params = {'query': from_, 'max_results': tweet_limit, 'tweet.fields': field_list}\n",
    "\n",
    "            json_response, response_headers = connect_to_endpoint(search_url, headers, query_params)\n",
    "\n",
    "            if \"status\" not in json_response and response_headers.get('x-rate-limit-remaining') is not None and int(response_headers.get('x-rate-limit-remaining')) > 0:\n",
    "                if 'result_count' in json_response['meta'] and json_response['meta']['result_count'] > 0:\n",
    "\n",
    "                    for el in json_response[\"data\"]:\n",
    "                        last_100_no_vax_tweets.insert_one(el)\n",
    "                        current_collected += 1\n",
    "                        total_collected += 1\n",
    "\n",
    "                print(\"Collected Tweets =\", current_collected,\"Total Tweets =\", total_collected)\n",
    "            else:\n",
    "                if response_headers.get('x-rate-limit-remaining') is not None:\n",
    "                    sleep_time = int(response_headers.get('x-rate-limit-reset')) - int(time.time())\n",
    "                    print(\"Sleeping for\",sleep_time + 1,\"s\")\n",
    "                    time.sleep(sleep_time + 1)\n",
    "                    print(\"I'm Awake\")\n",
    "\n",
    "    processed_count += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_count)/float(user_count) * 100), processed_count,\"/\", user_count)\n",
    "\n",
    "    if processed:\n",
    "        time.sleep(1)\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7b2cb1-2f65-4cf4-9569-d6eaae9abd2c",
   "metadata": {},
   "source": [
    "<h3>2.1 - Collect the original tweets of the retweets of the users classified as no-vax</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c9624-eefe-4002-a9d1-41142361bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url(ids):\n",
    "    tweet_fields = \"tweet.fields=\" + field_list\n",
    "    ids_list = \"ids=\" + ids\n",
    "    \n",
    "    url = \"https://api.twitter.com/2/tweets?{}&{}\".format(ids_list, tweet_fields)\n",
    "    \n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216fbe1d-2973-410e-a9e2-c636a8987c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_collected_original(tweet_id, main_collection, original_tweet):\n",
    "    query = {\"id\": tweet_id}\n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"original_tweet\": original_tweet\n",
    "        }\n",
    "    }\n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e159b00-c95a-4717-8d16-c242c75e284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_suspended_account_tweet(tweet_id, main_collection):\n",
    "    query = {\"id\": tweet_id}\n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"suspended\": True\n",
    "        }\n",
    "    }\n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9e414-50ab-44f6-bca7-cb9ea131e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_deleted_account_tweet(tweet_id, main_collection):\n",
    "    query = {\"id\": tweet_id}\n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"deleted\": True\n",
    "        }\n",
    "    }\n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7fbe3-2220-4875-a2ed-71b509926d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets = last_100_no_vax_tweets.find({\"referenced_tweets\": {\"$ne\": None}}, no_cursor_timeout = True)\n",
    "retweets_count = last_100_no_vax_tweets.count_documents({\"referenced_tweets\": {\"$ne\": None}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d92f3d-f367-46ac-ada7-750580d2cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = create_headers(bearer_token)\n",
    "\n",
    "request_ids = \"\"\n",
    "processed_count = 0\n",
    "ids_count = 0\n",
    "mappings = []\n",
    "\n",
    "# collect the original tweet from the referenced tweet\n",
    "for rt in retweets:\n",
    "    processed = False\n",
    "\n",
    "    if ('original_tweet' not in rt or not rt['original_tweet']) and ids_count < 100:\n",
    "\n",
    "        if request_ids != \"\":\n",
    "            request_ids += \",\"\n",
    "\n",
    "        request_ids += rt['referenced_tweets'][0]['id']\n",
    "        ids_count += 1\n",
    "\n",
    "        mappings.append({'retweet_id': rt['id'], 'original_tweet_id': rt['referenced_tweets'][0]['id'], 'elaborated': 0})\n",
    "\n",
    "    if ids_count == 100:\n",
    "        json_response, response_headers = connect_to_endpoint(create_url(request_ids), headers)\n",
    "\n",
    "        if \"status\" not in json_response and response_headers.get('x-rate-limit-remaining') is not None and int(response_headers.get('x-rate-limit-remaining')) > 0:\n",
    "\n",
    "            df = pd.DataFrame(mappings)\n",
    "\n",
    "            if \"data\" in json_response and len(json_response[\"data\"]) > 0:\n",
    "                for el in json_response[\"data\"]:\n",
    "\n",
    "                    matching_retweets = df.loc[(df['original_tweet_id'] == el['id']) & (df['elaborated'] == 0)]['retweet_id'].tolist()\n",
    "\n",
    "                    for m in matching_retweets:\n",
    "                        add_collected_original(m, last_100_no_vax_tweets, el)\n",
    "\n",
    "                    df.loc[df['original_tweet_id'] == el['id'], ['elaborated']] = 1\n",
    "\n",
    "            if \"errors\" in json_response and len(json_response[\"errors\"]) > 0:\n",
    "                for err in json_response[\"errors\"]:\n",
    "                    # suspended account\n",
    "                    if err['title'] == \"Authorization Error\":\n",
    "                        mark_deleted_account_tweet(err['value'], last_100_no_vax_tweets)\n",
    "\n",
    "                    # deleted account\n",
    "                    if err['title'] == \"Not Found Error\":\n",
    "                        mark_deleted_account_tweet(err['value'], last_100_no_vax_tweets)\n",
    "\n",
    "            ids_count = 0;\n",
    "            request_ids = \"\"\n",
    "            mappings = []\n",
    "            processed = True\n",
    "        else:\n",
    "            if response_headers.get('x-rate-limit-remaining') is not None:\n",
    "                sleep_time = int(response_headers.get('x-rate-limit-reset')) - int(time.time())\n",
    "                print(\"Sleeping for\",sleep_time + 1,\"s\")\n",
    "                time.sleep(sleep_time + 1)\n",
    "                print(\"I'm Awake\")\n",
    "\n",
    "    processed_count += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_count)/float(retweets_count) * 100), processed_count,\"/\", retweets_count)\n",
    "\n",
    "    if processed:\n",
    "        time.sleep(1)\n",
    "\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f28af-e61b-48af-bc19-5ae7c425c72f",
   "metadata": {},
   "source": [
    "<h3>3 - Integrate the bot score associated with the users from botometer</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07208f7e-8f89-4100-953b-73faabcae58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_bot_update(author_id, bot_score, main_collection):\n",
    "    query = {\"id\": author_id}\n",
    "    is_bot = False\n",
    "        \n",
    "    if bot_score >= 0.5:\n",
    "        is_bot = True\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"user_bot_score\": bot_score,\n",
    "            \"is_bot\": is_bot\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fca5d44-40e4-426f-82f9-2882aebd560e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'no_vax_users_bot_score.csv'\n",
    "csv = pd.read_csv (file_name, header = 0)\n",
    "df = pd.DataFrame(csv, columns= ['user_id', 'user_bot_score'])\n",
    "\n",
    "user_count = no_vax_users.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf084ba6-894e-47c3-841b-d17e27452d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_users = 0\n",
    "for row in csv.itertuples():\n",
    "    is_bot_update(int(row.user_id), float(row.user_bot_score), no_vax_users)\n",
    "    processed_users += 1\n",
    "    \n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_users)/float(user_count) * 100), processed_users,\"/\", user_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ccb3ff-ab6a-46d9-b7b7-64ca869eabe9",
   "metadata": {},
   "source": [
    "<h3>4 - Define whether a user is a no-vax or not depending on the collected content</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dffb2a-9f10-4c2b-832b-39cf2e00b5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_collected_tweet_count(author_id, count, main_collection):\n",
    "    query = {\"id\": author_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"collected_tweet_count\": count\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33124449-819b-4160-915b-fa4cfe44e4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider all the no-vax users for which I collected the data\n",
    "no_vax_users_retr = no_vax_users.find({})\n",
    "user_count = no_vax_users.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbc7d7-b0a2-4190-bd9a-305087795304",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_users = 0\n",
    "no_vax_tweets_authors = []\n",
    "\n",
    "# for each supposed no-vax user, store the number of tweets shared\n",
    "for user in no_vax_users_retr:\n",
    "    count_tweets = last_100_no_vax_tweets.count_documents({\"author_id\": str(user['id'])})\n",
    "    \n",
    "    if count_tweets > 0:\n",
    "        no_vax_tweets_authors.append(user)\n",
    "    \n",
    "    update_collected_tweet_count(user['id'], count_tweets, no_vax_users)\n",
    "    \n",
    "    processed_users += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_users)/float(user_count) * 100), processed_users,\"/\", user_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ac269-f6f8-4956-9690-3d7554a8aa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_tweet_frequency(author_id, main_collection, freq):\n",
    "    query = {\"id\": author_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"max_tweet_frequency_per_minute\": freq\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7e4d2b-2beb-4434-b450-300d6f34b729",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vax_users_retr = no_vax_users.find({})\n",
    "user_count = no_vax_users.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42315a39-a456-497d-8e9d-6a082bfb4fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the sharing frequency for each user\n",
    "processed_users = 0\n",
    "for user in no_vax_users_retr:\n",
    "    if user['collected_tweet_count'] > 0:\n",
    "        # get all the original tweet from the list of users for which there are tweets, ignoring the retweets\n",
    "        user_tweets = last_100_no_vax_tweets.find({\"author_id\": str(user['id']), \"referenced_tweets\": {\"$eq\": None}}).sort(\"created_at\", pymongo.DESCENDING)\n",
    "        \n",
    "        # to array\n",
    "        user_tweets_array = []\n",
    "        for tweet in user_tweets:\n",
    "            user_tweets_array.append(tweet)\n",
    "\n",
    "        if len(user_tweets_array) > 0:\n",
    "            user_tweet_frequency = []\n",
    "            while True:\n",
    "                # compute time range\n",
    "                recent_datetime = datetime.datetime.strptime(user_tweets_array[0]['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "                delta_time = datetime.timedelta(minutes = 1)\n",
    "                before_datetime = recent_datetime - delta_time\n",
    "\n",
    "                # count published tweets in that time range (count and remove them from array)\n",
    "                to_be_removed = []\n",
    "                published_tweets_count = 0\n",
    "                for tweet in user_tweets_array:\n",
    "                    if datetime.datetime.strptime(tweet['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\") >= before_datetime and datetime.datetime.strptime(tweet['created_at'], \"%Y-%m-%dT%H:%M:%S.%fZ\") <= recent_datetime:\n",
    "                        published_tweets_count += 1\n",
    "                        to_be_removed.append(tweet)\n",
    "\n",
    "                for tbr in to_be_removed:\n",
    "                    user_tweets_array.remove(tbr)\n",
    "\n",
    "                user_tweet_frequency.append(published_tweets_count)\n",
    "\n",
    "                if len(user_tweets_array) == 0:\n",
    "                    break\n",
    "\n",
    "            # update\n",
    "            set_tweet_frequency(user['id'], no_vax_users, numpy.max(user_tweet_frequency).item())\n",
    "    \n",
    "    processed_users += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_users)/float(empty_no_vax_users_count) * 100), processed_users,\"/\", empty_no_vax_users_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e9b5b-4c54-47f9-b02f-8a16369c8215",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_user(author_id, main_collection, boolean):\n",
    "    query = {\"id\": author_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"ignored\": boolean\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c9a42a-809b-4c17-b6e6-5f6052a0b7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_user_tweet(tweet_id, main_collection):\n",
    "    query = {\"id\": tweet_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"ignored\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62092e-66d5-444d-9cdd-ccde94684714",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_no_vax_users = no_vax_users.find({})\n",
    "to_be_ignored_users = no_vax_users.find({\"$or\":[{\"collected_tweet_count\": 0}, {\"verified\": True}, {\"is_bot\": True}, {\"max_tweet_frequency_per_minute\": {\"$gt\": 2}}, {\"no_vax_percentage\": {\"$lt\": 0.6}}]})\n",
    "to_be_ignored_users_count = no_vax_users.count_documents({\"$or\":[{\"collected_tweet_count\": 0}, {\"verified\": True}, {\"is_bot\": True}, {\"max_tweet_frequency_per_minute\": {\"$gt\": 2}}, {\"no_vax_percentage\": {\"$lt\": 0.6}}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeecee7-8540-4aa8-a27f-486e60a347db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore users that have either no shared tweets or are marked as bot or are verified users or have a tweet frequency greater\n",
    "# than 2 per minute or have a percentage of no-vax tweets shared < 0.6\n",
    "processed_users = 0\n",
    "\n",
    "for user in all_no_vax_users:\n",
    "    ignore_user(user['id'], no_vax_users, False)\n",
    "\n",
    "for user in to_be_ignored_users:\n",
    "    ignore_user(user['id'], no_vax_users, True)\n",
    "    \n",
    "    for user_tweet in last_100_no_vax_tweets.find({\"author_id\": str(user['id'])}):\n",
    "        ignore_user_tweet(user_tweet['id'], last_100_no_vax_tweets)\n",
    "    \n",
    "    processed_users += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_users)/float(to_be_ignored_users_count) * 100), processed_users,\"/\", to_be_ignored_users_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da099dbe-8af0-4166-a687-be2db727d06f",
   "metadata": {},
   "source": [
    "<h3>5 - Data cleaning step for topic analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ffd6a-f96f-48a5-b205-2d1187ad96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('italian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf59994-328b-4b96-9572-3a6eba8cdf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_vax_user_keep = no_vax_users.find({\"ignored\": False})\n",
    "no_vax_user_keep_count = no_vax_users.count_documents({\"ignored\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92fed3d-e12b-4211-a4ce-d54c0ec2020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new collection to simplify the analysis\n",
    "processed_count = 0\n",
    "for nvuk in no_vax_user_keep:\n",
    "    no_vax_users_cleaned.insert_one(nvuk)\n",
    "    \n",
    "    no_vax_tweets_keep = last_100_no_vax_tweets.find({\"author_id\": str(nvuk['id'])})\n",
    "    for tw in no_vax_tweets_keep:\n",
    "        last_100_no_vax_tweets_cleaned.insert_one(tw)\n",
    "        \n",
    "    processed_count += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_count)/float(no_vax_user_keep_count) * 100), processed_count,\"/\", no_vax_user_keep_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995af25-d1b9-4582-81dd-63b82d744817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(tweet):\n",
    "    \n",
    "    # remove quotes\n",
    "    quotes = re.findall(\"@[A-Za-z0-9]+\", tweet)\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\", \"\", tweet)\n",
    "    \n",
    "    # remove http links\n",
    "    urls = re.findall(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", tweet)\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet)\n",
    "    \n",
    "    # remove emojis\n",
    "    emojis = [c for c in tweet if c in emoji.UNICODE_EMOJI['en'] or c in emoji.UNICODE_EMOJI['it']]\n",
    "    tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI['en'] and c not in emoji.UNICODE_EMOJI['it'])\n",
    "    \n",
    "    # substitute apices with spaces\n",
    "    # tweet = tweet.replace(\"'\", \" \")\n",
    "    while True:\n",
    "        apices_pos = tweet.find(\"'\")\n",
    "        \n",
    "        if apices_pos == -1:\n",
    "            break\n",
    "            \n",
    "        sub_tweet = tweet[0:apices_pos]\n",
    "        last_space_index_before_apices = sub_tweet.rfind(\" \")\n",
    "        \n",
    "        if last_space_index_before_apices == -1:\n",
    "            last_space_index_before_apices = 0\n",
    "        \n",
    "        tweet = tweet[0:last_space_index_before_apices] + tweet[apices_pos + 1:len(tweet)]\n",
    "    \n",
    "    # remove punctuation including hashtag sign\n",
    "    tweet_with_hash_text = ''.join(c.lower() for c in tweet if c not in string.punctuation)\n",
    "    \n",
    "    # remove spaces\n",
    "    tweet_with_hash_text = ' '.join(tweet_with_hash_text.split())\n",
    "    \n",
    "    # tokenize\n",
    "    token_text_with_hash = re.split('\\W+', tweet_with_hash_text)\n",
    "    \n",
    "    token_text_with_hash_no_stop = [word for word in token_text_with_hash if word not in stopword and word != '']\n",
    "    \n",
    "    # remove hashtags\n",
    "    hashtags = re.findall(\"#[^ ]*\", tweet)\n",
    "    tweet = re.sub(\"#[^ ]*\", \"\", tweet)\n",
    "    \n",
    "    # cleaned tweet with punctuation\n",
    "    tweet = tweet.strip()\n",
    "    \n",
    "    # remove punctuation\n",
    "    no_punct_tweet = ''.join(c.lower() for c in tweet if c not in string.punctuation)\n",
    "    \n",
    "    no_punct_tweet = ' '.join(no_punct_tweet.split())\n",
    "    \n",
    "    # re-organize and stemming content\n",
    "    text_lc = \"\".join([word.lower() for word in no_punct_tweet])\n",
    "    \n",
    "    text_rc = re.sub('[0-9]+', '', text_lc)\n",
    "    \n",
    "    token_text = re.split('\\W+', text_rc) # tokenization\n",
    "    \n",
    "    token_text = [word for word in token_text if word not in stopword and word != '']  # remove stopwords\n",
    "     \n",
    "    return quotes, urls, emojis, hashtags, token_text, token_text_with_hash_no_stop, no_punct_tweet, tweet_with_hash_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3483bb-7f9f-48bd-829a-3817c9f066fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_tweet(tweet_id, main_collection, extracted_quotes, extracted_urls, extracted_emojis, extracted_hashtags, extracted_token_text_no_hashtags, extracted_token_text_with_hashtags, extracted_no_punct_tweet, extracted_tweet_with_hash_text):\n",
    "    query = {\"id\": tweet_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"extracted_quotes\": extracted_quotes,\n",
    "            \"extracted_urls\": extracted_urls,\n",
    "            \"extracted_emojis\": extracted_emojis,\n",
    "            \"extracted_hashtags\": extracted_hashtags,\n",
    "            \"extracted_token_text_no_hashtags\": extracted_token_text_no_hashtags,\n",
    "            \"extracted_token_text_with_hashtags\": extracted_token_text_with_hashtags,\n",
    "            \"extracted_clean_tweet_no_hashtags\": extracted_no_punct_tweet,\n",
    "            \"extracted_clean_tweet_with_hashtags\": extracted_tweet_with_hash_text,\n",
    "            \"has_text_processed\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088adcd2-c400-4abf-9d4c-36a905dd0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_has_text_processed(tweet_id, main_collection):\n",
    "    query = {\"id\": tweet_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"has_text_processed\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d516dd4-ff22-4025-8109-8919be0c68c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = last_100_no_vax_tweets_cleaned.find({}, no_cursor_timeout = True)\n",
    "all_tweets_count = last_100_no_vax_tweets_cleaned.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09c3f3d-9b7f-472e-8672-64109f2ae54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup a new column\n",
    "pre_tweet_processed_count = 0\n",
    "for tweet in all_tweets:\n",
    "    add_has_text_processed(tweet['id'], last_100_no_vax_tweets_cleaned)\n",
    "    \n",
    "    pre_tweet_processed_count += 1\n",
    "    print(\"Pre-Processed ({:.2f} %) -\".format(float(pre_tweet_processed_count)/float(all_tweets_count) * 100), pre_tweet_processed_count,\"/\", all_tweets_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0d7ebb-266e-4f34-88bb-92c5ac447025",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = last_100_no_vax_tweets_cleaned.find({\"has_text_processed\": {\"$eq\": False}}, no_cursor_timeout = True)\n",
    "all_tweets_count = last_100_no_vax_tweets_cleaned.count_documents({\"has_text_processed\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1dfa36-810d-4e31-8629-33b07b2275c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning for each tweet\n",
    "tweet_processed_count = 0\n",
    "for tweet in all_tweets:\n",
    "    if 'referenced_tweets' in tweet and tweet['referenced_tweets'][0]['type'] == 'retweeted':\n",
    "        if 'original_tweet' in tweet:\n",
    "            t_quotes, t_urls, t_emojis, t_hashtags, t_token_text, t_token_text_with_hash, t_no_punct_tweet, t_tweet_with_hash_text = text_cleaner(tweet['original_tweet']['text'])\n",
    "            update_tweet(tweet['id'], last_100_no_vax_tweets_cleaned, t_quotes, t_urls, t_emojis, t_hashtags, t_token_text, t_token_text_with_hash, t_no_punct_tweet, t_tweet_with_hash_text)\n",
    "    else:\n",
    "        t_quotes, t_urls, t_emojis, t_hashtags, t_token_text, t_token_text_with_hash, t_no_punct_tweet, t_tweet_with_hash_text = text_cleaner(tweet['text'])\n",
    "        update_tweet(tweet['id'], last_100_no_vax_tweets_cleaned, t_quotes, t_urls, t_emojis, t_hashtags, t_token_text, t_token_text_with_hash, t_no_punct_tweet, t_tweet_with_hash_text)\n",
    "\n",
    "    tweet_processed_count += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(tweet_processed_count)/float(all_tweets_count) * 100), tweet_processed_count,\"/\", all_tweets_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6a46fa-729c-4e48-8046-103e5cf00aa2",
   "metadata": {},
   "source": [
    "<h5><b>Before running the following steps of the code, be sure that the previous one has been correctly performed</b></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a443a5-4d87-42d1-a5e1-57c35ffc65a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = last_100_no_vax_tweets_cleaned.find({\"has_text_processed\": {\"$eq\": False}}, no_cursor_timeout = True)\n",
    "all_tweets_count = last_100_no_vax_tweets_cleaned.count_documents({\"has_text_processed\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38e020e-4683-40c9-97bd-b0850083dfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the tweets that couldn't be processed\n",
    "tweet_processed_count = 0\n",
    "for tweet in all_tweets:\n",
    "    last_100_no_vax_tweets_cleaned.delete_one({\"id\": tweet[\"id\"]})\n",
    "    \n",
    "    tweet_processed_count += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(tweet_processed_count)/float(all_tweets_count) * 100), tweet_processed_count,\"/\", all_tweets_count)\n",
    "    clear_output(wait=True)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0703a4ec-1ba7-467c-8a4d-f0aa019d7096",
   "metadata": {},
   "source": [
    "<h5><b>-----------------------------------------------------------------------------------------------------------------------------</b></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc819d2-4ff2-4cd1-9cc2-2b597aea3757",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"it_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413bebd5-af0a-41d3-a7ba-f17309a225a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_has_text_processed(tweet_id, main_collection):\n",
    "    query = {\"id\": tweet_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"has_text_processed\": False\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f0991-21ce-4671-a04d-4c4305900a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_reset_tweets = last_100_no_vax_tweets_cleaned.find({})\n",
    "to_be_reset_tweets_count = last_100_no_vax_tweets_cleaned.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7db02-ef2f-4fb3-be02-dfc87b5580df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the text processed field\n",
    "processed_tweets = 0\n",
    "for tweet in to_be_reset_tweets:\n",
    "    reset_has_text_processed(tweet['id'], new_tweets_cleaned_collection)\n",
    "    \n",
    "    processed_tweets += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_tweets)/float(to_be_reset_tweets_count) * 100), processed_tweets,\"/\", to_be_reset_tweets_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd15172-fdb2-4d89-9bde-fc017a5b21b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_useful_tokens_for_ta(tweet_id, main_collection, tokens_for_topic_analysis_no_verbs, tokens_for_topic_analysis):\n",
    "    query = {\"id\": tweet_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"tokens_for_topic_analysis_no_verbs\": list(set(tokens_for_topic_analysis_no_verbs)),\n",
    "            \"tokens_for_topic_analysis\": list(set(tokens_for_topic_analysis)),\n",
    "            \"has_text_processed\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97704c4-c51a-4857-a306-21b6ee06dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = last_100_no_vax_tweets_cleaned.find({\"has_text_processed\": {\"$eq\": False}})\n",
    "all_tweets_count = last_100_no_vax_tweets_cleaned.count_documents({\"has_text_processed\": {\"$eq\": False}})\n",
    "excluded_words = ['cose', 'cosa', 'merda', 'cazzo', 'coglioni', 'culo', '\\u200d️']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c953874a-3a90-4ed9-8f02-f529dccd7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the words for the topic analysis\n",
    "processed_tweets = 0\n",
    "for tweet in all_tweets:\n",
    "    token_array = []\n",
    "    token_w_verbs_array = []\n",
    "    doc = nlp(tweet['extracted_clean_tweet_no_hashtags'])\n",
    "    \n",
    "    for token in doc:\n",
    "        if len(token.text) > 1:\n",
    "            if token.lemma_ not in excluded_words:\n",
    "                if token.pos_ == \"PROPN\" or token.pos_ == \"NOUN\":\n",
    "                    token_array.append(token.text)\n",
    "                    token_w_verbs_array.append(token.text)\n",
    "\n",
    "                if token.pos_ == \"VERB\" and token.lemma_ != \"essere\" and token.lemma_ != \"avere\":\n",
    "                    token_w_verbs_array.append(token.lemma_)\n",
    "    \n",
    "    set_useful_tokens_for_ta(tweet['id'], last_100_no_vax_tweets_cleaned, token_array, token_w_verbs_array)\n",
    "    \n",
    "    processed_tweets += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_tweets)/float(all_tweets_count) * 100), processed_tweets,\"/\", all_tweets_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cb7473-0494-4a93-96de-15d5f15fa953",
   "metadata": {},
   "source": [
    "<h3>6 - Topic analysis</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bae9b5b-d00c-4bfd-ab58-a564a3e73d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_for_topic_analysis = last_100_no_vax_tweets_cleaned.find({\"has_text_processed\": {\"$eq\": True}}, no_cursor_timeout = True) \n",
    "tweets_for_topic_analysis_count = last_100_no_vax_tweets_cleaned.count_documents({\"has_text_processed\": {\"$eq\": True}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2b3a3-e323-4a40-8531-2dbbe0fcfd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "processed_tweets = 0\n",
    "\n",
    "# collect the tweets with at least two tokens and whose language is italian\n",
    "for tweet in tweets_for_topic_analysis:\n",
    "    if len(tweet['tokens_for_topic_analysis_no_verbs']) > 2 and tweet['lang'] == \"it\":\n",
    "        texts.append(tweet['tokens_for_topic_analysis_no_verbs'])\n",
    "    \n",
    "    processed_tweets += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_tweets)/float(tweets_for_topic_analysis_count) * 100), processed_tweets,\"/\", tweets_for_topic_analysis_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b9c640-ae0a-4d44-a8de-5380223e5167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the corpus\n",
    "id2word = corpora.Dictionary(texts)\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b4e221-0a17-4fb2-a781-6bf58ee4ddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic analysis with varying number of topics (from lower_range to upper_range)\n",
    "lower_range = 3\n",
    "upper_range = 30\n",
    "step = 1\n",
    "coherence_values = []\n",
    "model_list = []\n",
    "\n",
    "for n_t in range(lower_range, upper_range, step):\n",
    "    print(\"Topics\", n_t)\n",
    "    model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=n_t,\n",
    "                                       random_state=1)\n",
    "    model_list.append(model)\n",
    "    coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, coherence='c_v')\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94545acb-149a-40bb-8a01-4c4f5ce79fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plom the coherence scores\n",
    "x = range(lower_range, upper_range, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5502f-21c8-497a-8fc3-0d300fd3d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"C:/Users/Andrea/Desktop/Vaccinitaly Analysis/models/all_lda_3_to_30/\"\n",
    "file_name = \"lda_\"\n",
    "extension = \".model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1607c-1cd1-459b-b614-56cd9ca465f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store all the coherence scores and the models\n",
    "i = 3\n",
    "for lda_model in model_list:\n",
    "    temp_file = datapath(folder + file_name + str(i) + extension)\n",
    "    lda_model.save(temp_file)\n",
    "    i += 1\n",
    "\n",
    "f = open(folder + \"coherence_scores_3_to_30.txt\", \"w\")\n",
    "for c_v in coherence_values:\n",
    "    f.write(str(c_v) + \"\\n\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879308be-ab52-4198-92a4-aa27826fbd67",
   "metadata": {},
   "source": [
    "<h3>6.1 - Best model in Topic analysis</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22468b07-7f47-4664-b184-d75cd4b2ccc9",
   "metadata": {},
   "source": [
    "<h5><b>Before running the following steps of the code configure them depending on the previous results</b></h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff2ea57-c927-4463-907f-d1bf85c45ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02449b-4937-4edc-8a83-06ef7d81f91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the model to consider\n",
    "n_topic = 16\n",
    "\n",
    "# setup the number of tweets to display to help with topic naming\n",
    "MAX_TWEETS = 5\n",
    "\n",
    "# setup the number of words to print for each topic\n",
    "n_words_per_topic = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5585c-9ed2-4cea-8c93-c6c83b61827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model with the desired amount of topics\n",
    "model_path = datapath(folder + file_name + n_topic + extension)\n",
    "best_model = LdaModel.load(model_path)\n",
    "\n",
    "x = best_model.show_topics(num_topics=n_topic, num_words=n_words_per_topic, formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "# print the top n_words_per_topic words\n",
    "for topic in topics_words:\n",
    "    print(topic[0], topic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b42c826-60a3-474b-ab4f-e7cb50fafc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data if needed\n",
    "if id2word is None or corpus is None:\n",
    "    tweets_for_topic_analysis = last_100_no_vax_tweets_cleaned.find({\"has_text_processed\": {\"$eq\": True}}, no_cursor_timeout = True) \n",
    "    tweets_for_topic_analysis_count = last_100_no_vax_tweets_cleaned.count_documents({\"has_text_processed\": {\"$eq\": True}})\n",
    "\n",
    "    id2word = corpora.Dictionary(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "    texts = []\n",
    "    processed_tweets = 0\n",
    "\n",
    "    # collect the tweets with at least two tokens and whose language is italian\n",
    "    for tweet in tweets_for_topic_analysis:\n",
    "        if len(tweet['tokens_for_topic_analysis_no_verbs']) > 2 and tweet['lang'] == \"it\":\n",
    "            texts.append(tweet['tokens_for_topic_analysis_no_verbs'])\n",
    "\n",
    "        processed_tweets += 1\n",
    "        print(\"Processed ({:.2f} %) -\".format(float(processed_tweets)/float(tweets_for_topic_analysis_count) * 100), processed_tweets,\"/\", tweets_for_topic_analysis_count)\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    id2word = corpora.Dictionary(texts)\n",
    "    corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292b88ea-3f02-4432-a4ee-a4be0670ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the topics on graph\n",
    "vis_data = gensimvis.prepare(best_model, corpus, id2word)\n",
    "pyLDAvis.display(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e430201-6655-4bc1-b4b1-26b6f30671e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_topic(tweet_id, main_collection, topic):\n",
    "    query = {\"id\": tweet_id}\n",
    "    \n",
    "    new_values = {\n",
    "        \"$set\": {\n",
    "            \"topic\": topic\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    main_collection.update_one(query, new_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d99f1-25cf-4a67-8983-9ddddb614d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_wo_topic = last_100_no_vax_tweets_cleaned.find({}, no_cursor_timeout = True) \n",
    "tweets_wo_topic_count = last_100_no_vax_tweets_cleaned.count_documents({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6df1ea-a4d7-407d-a881-cced8930bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "processed_tweets = 0\n",
    "for tw_t in tweets_wo_topic:\n",
    "    if len(tw_t['tokens_for_topic_analysis_no_verbs']) > 0 and tweet['lang'] == \"it\":\n",
    "        tw_t_id = id2word.doc2bow(tw_t['tokens_for_topic_analysis_no_verbs'])\n",
    "        doc_topics = best_model.get_document_topics(tw_t_id)\n",
    "        \n",
    "        equals = True\n",
    "        if len(doc_topics) > 1:\n",
    "            basic_topic_perc = doc_topics[0][1]\n",
    "            for item in doc_topics:\n",
    "                if basic_topic_perc != item[1]:\n",
    "                    equals = False\n",
    "                    break\n",
    "        if not equals:\n",
    "            val = max(doc_topics, key=itemgetter(1))\n",
    "            update_topic(tw_t['id'], new_tweets_cleaned_collection, [val[0], float(val[1])])\n",
    "        else:\n",
    "            update_topic(tw_t['id'], new_tweets_cleaned_collection, [\"Undefined\", \"Undefined\"])\n",
    "    else:\n",
    "        update_topic(tw_t['id'], new_tweets_cleaned_collection, [\"Undefined\", \"Undefined\"])\n",
    "\n",
    "    processed_tweets += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_tweets)/float(tweets_wo_topic_count) * 100), processed_tweets,\"/\", tweets_wo_topic_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ffdda-9386-4825-a2e3-21d6c6e5f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_collection_topic = last_100_no_vax_tweets_cleaned.find({\"topic.0\": {\"$ne\": \"Undefined\"}}, no_cursor_timeout = True)\n",
    "tweets_collection_topic_count = last_100_no_vax_tweets_cleaned.count_documents({\"topic.0\": {\"$ne\": \"Undefined\"}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ca28dd-e984-4b5a-9df3-2fcf0e78e169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most representative tweets per topic\n",
    "top_elements = []\n",
    "processed_tweets = 0\n",
    "\n",
    "for i in range(0, n_topic):\n",
    "    top_elements.append([])\n",
    "\n",
    "# find the top MAX_TWEETS representative tweets for each topic\n",
    "for tw_top in tweets_collection_topic:\n",
    "    # add if italian and no dups\n",
    "    if tw_top[\"lang\"] == \"it\" and tw_top[\"tokens_for_topic_analysis_no_verbs\"] not in [el[1] for el in top_elements[tw_top[\"topic\"][0]]]:\n",
    "        top_elements[tw_top[\"topic\"][0]].append([tw_top[\"topic\"], tw_top[\"extracted_clean_tweet_no_hashtags\"]])\n",
    "\n",
    "        if len(top_elements[tw_top[\"topic\"][0]]) > MAX_TWEETS:\n",
    "            tmp_top_el = top_elements[tw_top[\"topic\"][0]]\n",
    "            # sort\n",
    "            tmp_top_el = sorted(tmp_top_el, key=lambda el: el[0][1], reverse=True)\n",
    "            top_elements[tw_top[\"topic\"][0]] = tmp_top_el[:-1]\n",
    "    \n",
    "    processed_tweets += 1\n",
    "    print(\"Processed ({:.2f} %) -\".format(float(processed_tweets)/float(tweets_collection_topic_count) * 100), processed_tweets,\"/\", tweets_collection_topic_count)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c0b96-9e00-480a-af21-2efc98738de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the most representing tweets to help with the topic detection\n",
    "for el in top_elements:\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61d2707-fbd2-4e59-8a41-edd8ec620923",
   "metadata": {},
   "source": [
    "<h1>Outcomes</h1>\n",
    "<h4>Topic 0 - Vaccini e Società</h4>\n",
    "<b>Words: </b><span>['persone', 'anni', 'scuola', 'ragazzi', 'vita', 'casa', 'servizio', 'euro']</span>\n",
    "<h4>Topic 1 - Felicissima Sera - Pio e Amedeo (Aprile 2021)</h4>\n",
    "<b>Words: </b><span>['bisogno', 'faccia', 'pio', 'amedeo', 'anni', 'francesco', 'casa', 'mondo', 'pensiero']</span>\n",
    "<h4>Topic 2 - Coronavirus</h4>\n",
    "<b>Words: </b><span>['maggio', 'covid', 'aprile', 'italia', 'morti', 'ore', 'giorno', 'libertà', 'dati', 'casi']</span>\n",
    "<h4>Topic 3 - Fedez Contro Rai</h4>\n",
    "<b>Words: </b><span>['rai', 'sera', 'fedez', 'sistema', 'censura', 'nomi', 'palco', 'telefonata', 'tempo', 'canale']</span>\n",
    "<h4>Topic 4 - Scandalo Grillo</h4>\n",
    "<b>Words: </b><span>['figlio', 'grillo', 'gente', 'conte', 'ragazza', 'anni', 'padre', 'parola', 'draghi']</span>\n",
    "<h4>Topic 5 - Altro</h4>\n",
    "<b>Words: </b><span>['giorno', 'mondo', 'mamma', 'settimana', 'vaccino', 'mare', 'compleanno', 'anno', 'raga', 'anni']</span>\n",
    "<h4>Topic 6 - Repubblica e Società</h4>\n",
    "<b>Words: </b><span>['presidente', 'anni', 'prof', 'repubblica', 'solidarietà', 'bocca', 'piacere', 'profilo']</span>\n",
    "<h4>Topic 7 - Lega Salvini - Destra</h4>\n",
    "<b>Words: </b><span>['salvini', 'piazza', 'milano', 'forza', 'lega', 'canzone', 'claudio', 'meloni', 'italia']</span>\n",
    "<h4>Topic 8 - Conflitto Israele</h4>\n",
    "<b>Words: </b><span>['israele', 'palestinesi', 'articolo', 'guerra', 'bambini', 'parte', 'gaza', 'pace', 'giornalisti']</span>\n",
    "<h4>Topic 9 - Calcio Italiano</h4>\n",
    "<b>Words: </b><span>['punto', 'juve', 'squadra', 'anni', 'partita', 'milan', 'calcio', 'champions', 'napoli', 'gol']</span>\n",
    "<h4>Topic 10 - Legge DDL Zan</h4>\n",
    "<b>Words: </b><span>['persona', 'ddl', 'soldi', 'legge', 'zan', 'speranza', 'parte', 'anni', 'ministro']</span>\n",
    "<h4>Topic 11 - Politica</h4>\n",
    "<b>Words: </b><span>['testa', 'governo', 'anni', 'draghi', 'giulia', 'giorni', 'processo', 'paese', 'consiglio', 'salvini']</span>\n",
    "<h4>Topic 12 - Elezioni Sindaco Roma</h4>\n",
    "<b>Words: </b><span>['roma', 'pd', 'programma', 'raggi', 'sindaco', 'lavoro', 'anni', 'maestro', 'm5s', 'lavoratori']</span>\n",
    "<h4>Topic 13 - Renzi - Italiaviva</h4>\n",
    "<b>Words: </b><span>['renzi', 'diretta', 'servizi', 'minuti', 'matteo', 'video', 'incontro', 'anno', 'report', 'attività']</span>\n",
    "<h4>Topic 14 - Festa della Donna</h4>\n",
    "<b>Words: </b><span>['grazie', 'giornata', 'vita', 'foto', 'donna', 'casa', 'anni', 'parte', 'cuore', 'genere']</span>\n",
    "<h4>Topic 15 - Attività Sociali - Riaperture</h4>\n",
    "<b>Words: </b><span>['anni', 'calcio', 'italia', 'libro', 'maggio', 'concerto', 'anno', 'club']</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aedf74f-f83d-4ad2-87c3-cafee584f69a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
